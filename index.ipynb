{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7dcb96",
   "metadata": {},
   "source": [
    "# Chargement des fichiers sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b644d3b",
   "metadata": {},
   "source": [
    "### transformation PDF -> TXT avec PyPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa08c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargé : Warhammer 4 - Livre de base.pdf (67641992 caractères) .pdf)\n",
      "Le fichier TXT Warhammer 4 - Livre de base.txt existe déjà, pas de conversion nécessaire.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Import Files \n",
    "def file_loader(folder):\n",
    "    files = []\n",
    "    for i, file_name in enumerate(os.listdir(folder)):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            extension = os.path.splitext(file_name)[1].lower()\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                files.append({\n",
    "                    'name': file_name,\n",
    "                    'path': file_path,\n",
    "                    'extension': extension,\n",
    "                    'content': f.read(),\n",
    "                })\n",
    "            print(f\"Chargé : {files[i]['name']} ({len(files[i]['content'])} caractères) {files[i]['extension']})\")\n",
    "    return files\n",
    "\n",
    "# Convert PDF to TXT\n",
    "def pdf_to_txt(file, output_folder_path):\n",
    "    \"\"\"\n",
    "    Convert a PDF file (already loaded as a dict from file_loader) to TXT and save it in output_folder.\n",
    "    Returns the path to the TXT file.\n",
    "    \"\"\"\n",
    "    txt_name = os.path.splitext(file['name'])[0] + '.txt'\n",
    "    txt_path = os.path.join(output_folder_path, txt_name)\n",
    "    if file['extension'] == '.pdf':\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Conversion du PDF {file['name']} en TXT...\")\n",
    "            with open(file['path'], 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f_txt:\n",
    "                f_txt.write(text)\n",
    "            print(f\"Fichier TXT créé : {os.path.splitext(file['name'])[0]}.txt\")\n",
    "        else:\n",
    "            print(f\"Le fichier TXT {txt_name} existe déjà, pas de conversion nécessaire.\")\n",
    "        return txt_path\n",
    "    print(f\"Le fichier {file['name']} n'est pas un PDF, pas de conversion effectuée.\")\n",
    "    return None\n",
    "\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "transformed_sources_path = os.path.join(os.getcwd(), \"transformed_sources\")\n",
    "files = file_loader(sources_path)\n",
    "for file in files:\n",
    "    if(file['extension'] == '.pdf'):\n",
    "        pdf_to_txt(file, transformed_sources_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ad44a",
   "metadata": {},
   "source": [
    "### transformation PDF -> TXT avec pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1caa7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "# Import Files \n",
    "def file_loader(folder):\n",
    "    files = []\n",
    "    for i, file_name in enumerate(os.listdir(folder)):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            extension = os.path.splitext(file_name)[1].lower()\n",
    "            with open(file_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
    "                files.append({\n",
    "                    'name': file_name,\n",
    "                    'path': file_path,\n",
    "                    'extension': extension,\n",
    "                    'content': f.read(),\n",
    "                })\n",
    "            print(f\"Chargé : {files[i]['name']} ({len(files[i]['content'])} caractères) {files[i]['extension']})\")\n",
    "    return files\n",
    "\n",
    "# Convert PDF to TXT\n",
    "def pdf_to_txt(file, output_folder_path):\n",
    "    \"\"\"\n",
    "    Convert a PDF file (already loaded as a dict from file_loader) to TXT and save it in output_folder.\n",
    "    Returns the path to the TXT file.\n",
    "    \"\"\"\n",
    "    txt_name = os.path.splitext(file['name'])[0] + '.txt'\n",
    "    txt_path = os.path.join(output_folder_path, txt_name)\n",
    "    \n",
    "    if file['extension'] == '.pdf':\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Conversion du PDF {file['name']} en TXT...\")\n",
    "            text = \"\"\n",
    "            with pdfplumber.open(file['path']) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f_txt:\n",
    "                f_txt.write(text)\n",
    "            print(f\"Fichier TXT créé : {txt_name}\")\n",
    "        else:\n",
    "            print(f\"Le fichier TXT {txt_name} existe déjà, pas de conversion nécessaire.\")\n",
    "        return txt_path\n",
    "    \n",
    "    print(f\"Le fichier {file['name']} n'est pas un PDF, pas de conversion effectuée.\")\n",
    "    return None\n",
    "\n",
    "# Répertoires\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "transformed_sources_path = os.path.join(os.getcwd(), \"transformed_sources2\")\n",
    "\n",
    "# Traitement\n",
    "files = file_loader(sources_path)\n",
    "for file in files:\n",
    "    if file['extension'] == '.pdf':\n",
    "        pdf_to_txt(file, transformed_sources_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017fc49",
   "metadata": {},
   "source": [
    "# Découpage des fichiers txt en chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af218f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun nouveaux fichier n'a été détecté (dans 'transformed_sources/').\n",
      "0 fichiers ont été découpés en chunks (dans 'chunks/CS_1000_CO_200/').\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "# Paramètres des chunks\n",
    "SOURCES_DIR = \"transformed_sources\"                             # Dossier contenant les fichiers texte à découper\n",
    "CHUNK_SIZE = 1000                                                # Nombre de tokens par chunk\n",
    "CHUNK_OVERLAP = 200                                              # Nombre de tokens de chevauchement\n",
    "OUTPUT_DIR = f\"chunks/CS_{CHUNK_SIZE}_CO_{CHUNK_OVERLAP}\"       # Dossier de sortie pour les chunks\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")     # Utiliser l'encodeur de tokens de OpenAI (compatible Mistral)\n",
    "\n",
    "# Initialiser le découpeur de texte\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    encoding_name=\"cl100k_base\",\n",
    ")\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(path, config):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def chunk_files(new_files, old_files):\n",
    "    i = 0\n",
    "    for i, new_file in enumerate(new_files, start=1):\n",
    "        print(f\"Découpage de {new_file} en chunk de {CHUNK_SIZE} tokens...\")\n",
    "        chunk_file(os.path.join(SOURCES_DIR, new_file))\n",
    "\n",
    "    print(f\"{i} fichiers ont été découpés en chunks (dans '{OUTPUT_DIR}/').\")\n",
    "    # Enregistre les paramètres de découpage et les fichiers traités dans un fichier JSON\n",
    "    new_param = {\n",
    "        \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "        \"CHUNK_OVERLAP\": CHUNK_OVERLAP,\n",
    "        \"SOURCE_DIR\": SOURCES_DIR,\n",
    "        \"files\": old_files + new_files,\n",
    "    }\n",
    "    write_json(os.path.join(OUTPUT_DIR, \"param.json\"), new_param)\n",
    "\n",
    "# Découpe un fichier texte en chunks et les enregistre dans le dossier de sortie\n",
    "def chunk_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        with open(os.path.join(OUTPUT_DIR, f\"{filename}_chunk_{i}.txt\"), \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(chunk)\n",
    "\n",
    "\n",
    "# vérifie et compare les fichiers ayant déjà été découpés avec les nouveaux fichiers\n",
    "# afin de ne pas créer de doublons de chunks\n",
    "def chunk_cleaner(files):\n",
    "    param = read_json(os.path.join(OUTPUT_DIR, \"param.json\"))\n",
    "\n",
    "    p_chunk_size = param.get(\"CHUNK_SIZE\")\n",
    "    p_chunk_overlap = param.get(\"CHUNK_OVERLAP\")\n",
    "    p_sources_dir = param.get(\"SOURCE_DIR\")\n",
    "    p_files = param.get(\"files\")\n",
    "\n",
    "    if p_chunk_size == CHUNK_SIZE and p_chunk_overlap == CHUNK_OVERLAP and p_sources_dir == SOURCES_DIR:\n",
    "        removed_files = list(set(p_files) - set(files))\n",
    "        new_files = list(set(files) - set(p_files))\n",
    "        if removed_files:\n",
    "            print(f\"Les fichiers suivants ne sont plus présents : {removed_files}\")\n",
    "            nb_cleaned = chunk_eraser(removed_files)\n",
    "            print(f\"{nb_cleaned} fichiers de chunks ont été supprimés (dans '{OUTPUT_DIR}'/).\")\n",
    "            param[\"files\"] = [f for f in p_files if f not in removed_files]\n",
    "            write_json(os.path.join(OUTPUT_DIR, \"param.json\"), param)\n",
    "        if not new_files:\n",
    "            print(f\"Aucun nouveaux fichier n'a été détecté (dans '{SOURCES_DIR}/').\")\n",
    "        else:\n",
    "            print(f\"{len(new_files)} nouveaux fichiers ont été détectés : {new_files} (dans '{SOURCES_DIR}/').\")\n",
    "        return new_files, param[\"files\"]\n",
    "\n",
    "def chunk_eraser(diff):\n",
    "    nb_cleaned = 0\n",
    "    for file in diff:\n",
    "        prefix = os.path.splitext(file)[0]\n",
    "        for f in os.listdir(OUTPUT_DIR):\n",
    "            if f.startswith(prefix) and f.endswith(\".txt\"):\n",
    "                file_path = os.path.join(OUTPUT_DIR, f)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    nb_cleaned += 1\n",
    "    return nb_cleaned\n",
    "\n",
    "files = [f for f in os.listdir(SOURCES_DIR) if f.endswith(\".txt\")]\n",
    "old_files = []\n",
    "if os.path.isfile(os.path.join(OUTPUT_DIR, \"param.json\")):\n",
    "    files, old_files = chunk_cleaner(files)\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "chunk_files(files, old_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6c14",
   "metadata": {},
   "source": [
    "# Création de la base de données vectorielle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80a40b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture des chunks...\n",
      "Génération des embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:15<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion dans la base vectorielle...\n",
      "508 chunks ajoutés à la base vectorielle dans 'vector_db/all_MiniLM_L6_v2'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "CHUNKS_NAME = \"CS_1000_CO_200\"\n",
    "CHUNKS_DIR = f\"chunks/{CHUNKS_NAME}\"  # Dossier contenant les chunks\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_DB_DIR = f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\"\n",
    "\n",
    "# Init Chroma\n",
    "client = chromadb.PersistentClient(path=VECTOR_DB_DIR)\n",
    "collection = client.get_or_create_collection(CHUNKS_NAME)\n",
    "\n",
    "# Embedding model\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Lecture des chunks\n",
    "def load_chunks():\n",
    "    chunks = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    for file in os.listdir(CHUNKS_DIR):\n",
    "        if file.endswith(\".txt\"):\n",
    "            path = os.path.join(CHUNKS_DIR, file)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                chunks.append(content)\n",
    "                metadatas.append({\"filename\": file})\n",
    "                ids.append(str(uuid.uuid4()))  # identifiant unique\n",
    "    return chunks, metadatas, ids\n",
    "\n",
    "# Génération + insertion\n",
    "def embed_and_store():\n",
    "    print(\"Lecture des chunks...\")\n",
    "    texts, metadatas, ids = load_chunks()\n",
    "\n",
    "    print(\"Génération des embeddings...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True).tolist()\n",
    "\n",
    "    print(\"Insertion dans la base vectorielle...\")\n",
    "    collection.add(\n",
    "        documents=texts,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    print(f\"{len(texts)} chunks ajoutés à la base vectorielle dans '{VECTOR_DB_DIR}'.\")\n",
    "\n",
    "embed_and_store()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e864d304",
   "metadata": {},
   "source": [
    "### Affichage des collections existantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da70a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom : CS_1000_CO_100, ID : 1f389081-73c5-4daf-b838-bf88aa24ab5f\n",
      "Nom : CS_1000_CO_150, ID : 4ee75404-7ce2-49f3-be03-f455ef30b653\n",
      "Nom : CS_800_CO_80, ID : 53fcc4cb-7fb3-4cc9-8b49-e0eea43e1188\n",
      "Nom : CS_300_CO_60, ID : 9cae8747-44df-47d3-8054-851d6d71bad4\n",
      "Nom : CS_300_CO_30, ID : bf0699be-251c-45f6-9656-0a7d9c38bac6\n",
      "Nom : CS_800_CO_160, ID : c8e7ab35-faf7-4d49-bcab-a43898a80a8d\n",
      "Nom : CS_500_CO_100, ID : d2e7a108-12b2-4a48-8f9f-e83c371e60f5\n",
      "Nom : CS_1000_CO_200, ID : dd9a73c8-df80-4627-bd14-4d734ae5108f\n",
      "Nom : CS_500_CO_50, ID : e21c2070-abd4-4b08-8f56-efa70d4e1a00\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\")\n",
    "collections = client.list_collections()\n",
    "\n",
    "# client.delete_collection(\"CS_800_CO_160\")\n",
    "\n",
    "for col in collections:\n",
    "    print(f\"Nom : {col.name}, ID : {col.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d215ee5",
   "metadata": {},
   "source": [
    "# Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f20f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des passages les plus pertinents...\n",
      "\n",
      "\n",
      "--- Résultat 1 (Warhammer 4 - Livre de base_chunk_268.txt) - Similarité : 0.48% (0.995) ---\n",
      "à pleine \n",
      "vitesse vers l'ennemi le plus proche que vous pouvez voir afin \n",
      "d'attaquer. En général, la seule action que vous pouvez faire est \n",
      "un Test de Compétence d'Armes ou un Test d'Athlétisme pour \n",
      "atteindre un ennemi plus rapidement. De plus, vous pouvez faire \n",
      "un Test de Mêlée d'Action Gratuit à chaque Round pendant que \n",
      "vous lancez tout ce que vous avez dans vos attaques. Enfin, vous \n",
      "gagnez un bonus de +1 Bonus de Force, telle est votre férocité. \n",
      "Vous restez dans la Frénésie jusqu'à ce que tous les ennemis dans \n",
      "votre ligne de vue soient pacifiés, ou jusqu'à ce que vous receviez \n",
      "la condition Étourdie ou Inconsciente . Une fois votre Frénésie \n",
      "terminée, vous recevez immédiatement un état de Fatigue .  \n",
      "Haine (Cible)\n",
      "Vous êtes consumé par la Haine pour la Cible, qui est \n",
      "normalement un groupe de personnes ou de créatures, telles \n",
      "que'Hochlanders','Pieuvres des tourbières', ou'Esclaves'. Vous \n",
      "n'aurez jamais \n",
      "d'interaction sociale avec quelqu'un ou quelque \n",
      "chose que vous déteste\n",
      "\n",
      "--- Résultat 2 (Warhammer 4 - Livre de base_chunk_231.txt) - Similarité : 0.16% (0.998) ---\n",
      "attaque \n",
      "en combat rapproché gagne un bonus de +10 pour vous \n",
      "toucher.\n",
      "Une Condition Aveuglé est supprimée à la fin de chaque \n",
      "Round.\n",
      "Briser\n",
      "Vous êtes terrifié, vaincu, paniqué ou convaincu que vous allez \n",
      "mourir. A votre tour, votre Mouvement et votre Action doivent \n",
      "êtreutilisés pour fuir aussi vite que possible jusqu'à ce que vous soyez \n",
      "dans une bonne cachette hors de la vue de l'ennemi ; alors \n",
      "vous pouvez utiliser votre Action sur une compétence qui \n",
      "vous permet de vous cacher plus efficacement. Vous recevez \n",
      "également une pénalité de -10 à tous les tests qui n'impliquent \n",
      "pas de courir et de se cacher.\n",
      "Vous ne pouvez pas faire de Test de Ralliement pour éviter d'être \n",
      "Brisé si vous êtes Engagé avec un ennemi (voir page 159). Si vous \n",
      "n'êtes pas engagé, à la fin de chaque Round, vous pouvez \n",
      "tenter un Test de Calme pour éliminer une Condition Brisée,  \n",
      "chaque SL éliminant une Condition  Brisée  supplémentaire, et la \n",
      "Difficulté déterminée par les circonstances dans lesquelles vou\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "CHUNKS_NAME = \"CS_1000_CO_200\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_DB_DIR = VECTOR_DB_DIR = f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\"\n",
    "TOP_K = 10  # nombre de résultats à retourner\n",
    "\n",
    "# Initialisation du client et du modèle\n",
    "client = chromadb.PersistentClient(path=VECTOR_DB_DIR)\n",
    "collection = client.get_collection(CHUNKS_NAME)\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def ask_question():\n",
    "    # question = input(\"Pose ta question : \").strip()\n",
    "    # if not question:\n",
    "    #     print(\"Tu dois poser une question.\")\n",
    "    #     return\n",
    "    question = \"Je souhaite faire une attaque à l'épée sur un ennemi, comment dois-je procéder ?\"\n",
    "\n",
    "    print(\"Recherche des passages les plus pertinents...\\n\")\n",
    "\n",
    "    # Embedding de la question\n",
    "    query_embedding = model.encode([question])[0].tolist()\n",
    "\n",
    "    # Recherche vectorielle\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Filtrer les résultats pour exclure ceux avec une distance >= 1\n",
    "    filtered_docs = []\n",
    "    filtered_metas = []\n",
    "    filtered_dists = []\n",
    "    for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "        if dist < 1:\n",
    "            filtered_docs.append(doc)\n",
    "            filtered_metas.append(meta)\n",
    "            filtered_dists.append(dist)\n",
    "\n",
    "\n",
    "\n",
    "    # Affichage des résultats filtrés\n",
    "    for i, (doc, meta, dist) in enumerate(zip(filtered_docs, filtered_metas, filtered_dists)):\n",
    "        print(f\"\\n--- Résultat {i+1} ({meta['filename']}) - Similarité : {round((1 - dist) * 100, 2)}% ({round(dist, 3)}) ---\")\n",
    "        print(doc.strip()[:1000])\n",
    "\n",
    "    filtered_results = {\n",
    "        \"documents\": [filtered_docs],\n",
    "        \"metadatas\": [filtered_metas],\n",
    "        \"distances\": [filtered_dists]\n",
    "    }\n",
    "    return filtered_results, question\n",
    "        \n",
    "RESULTS, QUESTION = ask_question()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf785fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Réponse de l'IA ---\n",
      "Pour effectuer une attaque à l'épée contre un adversaire, vous devrez tout d'abord vérifier si vous êtes dans la portée de l'épée et que le cible soit visible. Si cela est le cas, vous pouvez faire un test de compétence des armes pour effectuer votre attaque. En tant que personnage en frénésie, vous bénéficiez d'un bonus de +1 force.\n",
      "\n",
      "   De plus, vous gagnez un bonus de +1 pour toutes vos attaques contre le groupe haïssé si vous avez la haine pour cette cible. Enfin, vous pouvez effectuer un test de melee d'action gratuit à chaque tour pour lancer une attaque supplémentaire sans action complète.\n",
      "\n",
      "   Si vous avez la préférence ou les préjugés contre le groupe ennemis, vous subirez une pénalité de -10 à tous vos tests de sociabilité envers ce groupe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_llm_with_context_ollam(question, context, model=\"mistral\"):\n",
    "    context = \"\\n\\n\".join(RESULTS[\"documents\"][0])\n",
    "    prompt = f\"\"\"Tu es un assistant intelligent portant sur le jeu de role sur table Warhammer Fantasy.\n",
    "    Ton role est de répondre aux questions concernant les règles du jeu ainsi que sur l'univers de Warhammer Fantasy.\n",
    "    Tu dois répondre de manière concise et précise, en te basant sur les informations fournies dans le contexte ci-dessous.\n",
    "\n",
    "    ### Contexte :\n",
    "\n",
    "    {context}\n",
    "\n",
    "\n",
    "    ### Question : \n",
    "\n",
    "    {question}\n",
    "    \n",
    "\n",
    "    ### Réponse :\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data = response.json()\n",
    "    return data[\"response\"]\n",
    "\n",
    "question = QUESTION\n",
    "context = \"\\n\\n\".join(RESULTS[\"documents\"][0])\n",
    "answer = get_llm_with_context_ollam(question, context)\n",
    "\n",
    "print(f\"\\n--- Réponse de l'IA ---\\n{answer.strip()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM1)",
   "language": "python",
   "name": "llm1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
