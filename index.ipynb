{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7dcb96",
   "metadata": {},
   "source": [
    "# Chargement des fichiers sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab311b",
   "metadata": {},
   "source": [
    "### Conversion des images du pdf en text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7822950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Remplacement sur une page...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'indirect_reference' and no __dict__ for setting new attributes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m new_content = content_data.replace(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/Im2 Do\u001b[39m\u001b[33m\"\u001b[39m, replacement)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Remplace le contenu de la page avec le nouveau stream\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m page[NameObject(\u001b[33m\"\u001b[39m\u001b[33m/Contents\u001b[39m\u001b[33m\"\u001b[39m)] = \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# V√©rifie que /F1 existe (n√©cessaire pour dessiner le texte)\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/Font\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m page[\u001b[33m\"\u001b[39m\u001b[33m/Resources\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timot\\anaconda3\\envs\\LLM1\\Lib\\site-packages\\PyPDF2\\_writer.py:213\u001b[39m, in \u001b[36mPdfWriter._add_object\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.indirect_reference  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m._objects.append(obj)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindirect_reference\u001b[49m = IndirectObject(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._objects), \u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj.indirect_reference\n",
      "\u001b[31mAttributeError\u001b[39m: 'bytes' object has no attribute 'indirect_reference' and no __dict__ for setting new attributes"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from PyPDF2.generic import (\n",
    "    DictionaryObject,\n",
    "    NumberObject,\n",
    "    NameObject,\n",
    "    ArrayObject,\n",
    "    StreamObject,\n",
    "    TextStringObject,\n",
    "    IndirectObject\n",
    ")\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "INPUT_PDF = \"output_uncompressed.pdf\"\n",
    "OUTPUT_PDF = sources_path + \"/modified.pdf\"\n",
    "\n",
    "reader = PdfReader(sources_path + '/' + INPUT_PDF)\n",
    "writer = PdfWriter()\n",
    "\n",
    "# ID de la police √† r√©utiliser pour √©crire le texte \"[crane]\"\n",
    "FONT_REF = IndirectObject(120, 0, reader)  # correspond √† /C0_0 120 0 R\n",
    "\n",
    "# Cr√©e un Form XObject qui affiche le texte [crane]\n",
    "form_stream = StreamObject()\n",
    "form_stream._data = b\"BT /F1 12 Tf 0 0 Td ([crane]) Tj ET\"\n",
    "form_stream.update({\n",
    "    NameObject(\"/Type\"): NameObject(\"/XObject\"),\n",
    "    NameObject(\"/Subtype\"): NameObject(\"/Form\"),\n",
    "    NameObject(\"/BBox\"): ArrayObject([NumberObject(0), NumberObject(0), NumberObject(100), NumberObject(20)]),\n",
    "    NameObject(\"/Resources\"): DictionaryObject({\n",
    "        NameObject(\"/Font\"): DictionaryObject({\n",
    "            NameObject(\"/F1\"): FONT_REF\n",
    "        })\n",
    "    }),\n",
    "})\n",
    "\n",
    "# Ajoute le nouvel objet au PDF\n",
    "form_obj_number = writer._objects.__len__()\n",
    "writer._add_object(form_stream)\n",
    "replacement_ref = IndirectObject(form_obj_number, 0, writer)\n",
    "\n",
    "for page in reader.pages:\n",
    "    resources = page.get(\"/Resources\")\n",
    "    xobjects = resources.get(\"/XObject\")\n",
    "\n",
    "    if not xobjects:\n",
    "        writer.add_page(page)\n",
    "        continue\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    # Remplace l'entr√©e /Im2 si elle pointe vers 1326 0 R\n",
    "    new_xobject_dict = DictionaryObject()\n",
    "    for key, obj in xobjects.items():\n",
    "        if key == NameObject(\"/Im2\") and isinstance(obj, IndirectObject):\n",
    "            if obj.idnum == 1326:\n",
    "                new_xobject_dict[NameObject(\"/Im2\")] = replacement_ref\n",
    "                modified = True\n",
    "                continue\n",
    "        new_xobject_dict[key] = obj\n",
    "\n",
    "    if modified:\n",
    "        new_resources = DictionaryObject(resources)\n",
    "        new_resources[NameObject(\"/XObject\")] = new_xobject_dict\n",
    "        page[NameObject(\"/Resources\")] = new_resources\n",
    "\n",
    "    writer.add_page(page)\n",
    "\n",
    "with open(OUTPUT_PDF, \"wb\") as f:\n",
    "    writer.write(f)\n",
    "\n",
    "print(\"‚úÖ Fichier modifi√© enregistr√© dans :\", OUTPUT_PDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be187130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from PyPDF2.generic import (\n",
    "    DictionaryObject,\n",
    "    NumberObject,\n",
    "    NameObject,\n",
    "    ArrayObject,\n",
    "    StreamObject,\n",
    "    TextStringObject,\n",
    "    IndirectObject\n",
    ")\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "INPUT_PDF = \"output_uncompressed.pdf\"\n",
    "OUTPUT_PDF = sources_path + \"/modified.pdf\"\n",
    "\n",
    "reader = PdfReader(sources_path + '/' + INPUT_PDF)\n",
    "writer = PdfWriter()\n",
    "\n",
    "# ID de la police √† r√©utiliser pour √©crire le texte \"[crane]\"\n",
    "FONT_REF = IndirectObject(120, 0, reader)  # correspond √† /C0_0 120 0 R\n",
    "\n",
    "for page in reader.pages:\n",
    "    content_object = page.get_contents()\n",
    "    if not content_object:\n",
    "        writer.add_page(page)\n",
    "        continue\n",
    "\n",
    "    content_data = b\"\"\n",
    "    if isinstance(content_object, list):\n",
    "        for obj in content_object:\n",
    "            content_data += obj.get_data()\n",
    "    else:\n",
    "        content_data = content_object.get_data()\n",
    "\n",
    "     # Remplace l'image /Im2 Do par un affichage de texte\n",
    "    # Instruction PDF pour afficher du texte : BT /F1 12 Tf 0 0 Td ([crane]) Tj ET\n",
    "    # Attention, cela suppose qu'une police /F1 existe dans les /Resources\n",
    "\n",
    "    if b\"/Im2 Do\" in content_data:\n",
    "        print(\"üîÅ Remplacement sur une page\")\n",
    "\n",
    "        # Assure que la font /F1 existe dans les ressources\n",
    "        resources = page[\"/Resources\"]\n",
    "        if \"/Font\" not in resources:\n",
    "            resources[NameObject(\"/Font\")] = DictionaryObject()\n",
    "\n",
    "        font_dict = resources[\"/Font\"]\n",
    "        if \"/F1\" not in font_dict:\n",
    "            # Utilise une police existante dans la page, ici on utilise C0_0\n",
    "            if \"/C0_0\" in font_dict:\n",
    "                font_dict[NameObject(\"/F1\")] = font_dict[\"/C0_0\"]\n",
    "            else:\n",
    "                # On abandonne si aucune font n'existe\n",
    "                print(\"‚ö†Ô∏è Aucune font trouv√©e pour ins√©rer le texte.\")\n",
    "                writer.add_page(page)\n",
    "                continue\n",
    "        \n",
    "        # Construction d'une instruction texte en PDF\n",
    "        # Tu peux ajuster la position (Td) et la taille (Tf)\n",
    "        replacement = b\"BT /F1 12 Tf 10 10 Td ([crane]) Tj ET\"\n",
    "\n",
    "        # Remplace toutes les occurrences\n",
    "        modified_content = content_data.replace(b\"/Im2 Do\", replacement)\n",
    "\n",
    "        # Injecte le contenu modifi√© dans un nouveau stream\n",
    "        page.__setitem__(NameObject(\"/Contents\"), writer._add_stream(modified_content))\n",
    "\n",
    "    writer.add_page(page)\n",
    "\n",
    "with open(OUTPUT_PDF, \"wb\") as f:\n",
    "    writer.write(f)\n",
    "\n",
    "print(\"‚úÖ Fichier modifi√© enregistr√© dans :\", OUTPUT_PDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b644d3b",
   "metadata": {},
   "source": [
    "### transformation PDF -> TXT avec PyPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa08c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charg√© : Warhammer 4 - Livre de base.pdf (67641992 caract√®res) .pdf)\n",
      "Le fichier TXT Warhammer 4 - Livre de base.txt existe d√©j√†, pas de conversion n√©cessaire.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Import Files \n",
    "def file_loader(folder):\n",
    "    files = []\n",
    "    for i, file_name in enumerate(os.listdir(folder)):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            extension = os.path.splitext(file_name)[1].lower()\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                files.append({\n",
    "                    'name': file_name,\n",
    "                    'path': file_path,\n",
    "                    'extension': extension,\n",
    "                    'content': f.read(),\n",
    "                })\n",
    "            print(f\"Charg√© : {files[i]['name']} ({len(files[i]['content'])} caract√®res) {files[i]['extension']})\")\n",
    "    return files\n",
    "\n",
    "# Convert PDF to TXT\n",
    "def pdf_to_txt(file, output_folder_path):\n",
    "    \"\"\"\n",
    "    Convert a PDF file (already loaded as a dict from file_loader) to TXT and save it in output_folder.\n",
    "    Returns the path to the TXT file.\n",
    "    \"\"\"\n",
    "    txt_name = os.path.splitext(file['name'])[0] + '.txt'\n",
    "    txt_path = os.path.join(output_folder_path, txt_name)\n",
    "    if file['extension'] == '.pdf':\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Conversion du PDF {file['name']} en TXT...\")\n",
    "            with open(file['path'], 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f_txt:\n",
    "                f_txt.write(text)\n",
    "            print(f\"Fichier TXT cr√©√© : {os.path.splitext(file['name'])[0]}.txt\")\n",
    "        else:\n",
    "            print(f\"Le fichier TXT {txt_name} existe d√©j√†, pas de conversion n√©cessaire.\")\n",
    "        return txt_path\n",
    "    print(f\"Le fichier {file['name']} n'est pas un PDF, pas de conversion effectu√©e.\")\n",
    "    return None\n",
    "\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "transformed_sources_path = os.path.join(os.getcwd(), \"transformed_sources\")\n",
    "files = file_loader(sources_path)\n",
    "for file in files:\n",
    "    if(file['extension'] == '.pdf'):\n",
    "        pdf_to_txt(file, transformed_sources_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ad44a",
   "metadata": {},
   "source": [
    "### transformation PDF -> TXT avec pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1caa7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "# Import Files \n",
    "def file_loader(folder):\n",
    "    files = []\n",
    "    for i, file_name in enumerate(os.listdir(folder)):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            extension = os.path.splitext(file_name)[1].lower()\n",
    "            with open(file_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
    "                files.append({\n",
    "                    'name': file_name,\n",
    "                    'path': file_path,\n",
    "                    'extension': extension,\n",
    "                    'content': f.read(),\n",
    "                })\n",
    "            print(f\"Charg√© : {files[i]['name']} ({len(files[i]['content'])} caract√®res) {files[i]['extension']})\")\n",
    "    return files\n",
    "\n",
    "# Convert PDF to TXT\n",
    "def pdf_to_txt(file, output_folder_path):\n",
    "    \"\"\"\n",
    "    Convert a PDF file (already loaded as a dict from file_loader) to TXT and save it in output_folder.\n",
    "    Returns the path to the TXT file.\n",
    "    \"\"\"\n",
    "    txt_name = os.path.splitext(file['name'])[0] + '.txt'\n",
    "    txt_path = os.path.join(output_folder_path, txt_name)\n",
    "    \n",
    "    if file['extension'] == '.pdf':\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Conversion du PDF {file['name']} en TXT...\")\n",
    "            text = \"\"\n",
    "            with pdfplumber.open(file['path']) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f_txt:\n",
    "                f_txt.write(text)\n",
    "            print(f\"Fichier TXT cr√©√© : {txt_name}\")\n",
    "        else:\n",
    "            print(f\"Le fichier TXT {txt_name} existe d√©j√†, pas de conversion n√©cessaire.\")\n",
    "        return txt_path\n",
    "    \n",
    "    print(f\"Le fichier {file['name']} n'est pas un PDF, pas de conversion effectu√©e.\")\n",
    "    return None\n",
    "\n",
    "# R√©pertoires\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "transformed_sources_path = os.path.join(os.getcwd(), \"transformed_sources2\")\n",
    "\n",
    "# Traitement\n",
    "files = file_loader(sources_path)\n",
    "for file in files:\n",
    "    if file['extension'] == '.pdf':\n",
    "        pdf_to_txt(file, transformed_sources_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017fc49",
   "metadata": {},
   "source": [
    "# D√©coupage des fichiers txt en chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af218f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun nouveaux fichier n'a √©t√© d√©tect√© (dans 'transformed_sources/').\n",
      "0 fichiers ont √©t√© d√©coup√©s en chunks (dans 'chunks/CS_1000_CO_200/').\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "# Param√®tres des chunks\n",
    "SOURCES_DIR = \"transformed_sources\"                             # Dossier contenant les fichiers texte √† d√©couper\n",
    "CHUNK_SIZE = 1000                                                # Nombre de tokens par chunk\n",
    "CHUNK_OVERLAP = 200                                              # Nombre de tokens de chevauchement\n",
    "OUTPUT_DIR = f\"chunks/CS_{CHUNK_SIZE}_CO_{CHUNK_OVERLAP}\"       # Dossier de sortie pour les chunks\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")     # Utiliser l'encodeur de tokens de OpenAI (compatible Mistral)\n",
    "\n",
    "# Initialiser le d√©coupeur de texte\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    encoding_name=\"cl100k_base\",\n",
    ")\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(path, config):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def chunk_files(new_files, old_files):\n",
    "    i = 0\n",
    "    for i, new_file in enumerate(new_files, start=1):\n",
    "        print(f\"D√©coupage de {new_file} en chunk de {CHUNK_SIZE} tokens...\")\n",
    "        chunk_file(os.path.join(SOURCES_DIR, new_file))\n",
    "\n",
    "    print(f\"{i} fichiers ont √©t√© d√©coup√©s en chunks (dans '{OUTPUT_DIR}/').\")\n",
    "    # Enregistre les param√®tres de d√©coupage et les fichiers trait√©s dans un fichier JSON\n",
    "    new_param = {\n",
    "        \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "        \"CHUNK_OVERLAP\": CHUNK_OVERLAP,\n",
    "        \"SOURCE_DIR\": SOURCES_DIR,\n",
    "        \"files\": old_files + new_files,\n",
    "    }\n",
    "    write_json(os.path.join(OUTPUT_DIR, \"param.json\"), new_param)\n",
    "\n",
    "# D√©coupe un fichier texte en chunks et les enregistre dans le dossier de sortie\n",
    "def chunk_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        with open(os.path.join(OUTPUT_DIR, f\"{filename}_chunk_{i}.txt\"), \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(chunk)\n",
    "\n",
    "\n",
    "# v√©rifie et compare les fichiers ayant d√©j√† √©t√© d√©coup√©s avec les nouveaux fichiers\n",
    "# afin de ne pas cr√©er de doublons de chunks\n",
    "def chunk_cleaner(files):\n",
    "    param = read_json(os.path.join(OUTPUT_DIR, \"param.json\"))\n",
    "\n",
    "    p_chunk_size = param.get(\"CHUNK_SIZE\")\n",
    "    p_chunk_overlap = param.get(\"CHUNK_OVERLAP\")\n",
    "    p_sources_dir = param.get(\"SOURCE_DIR\")\n",
    "    p_files = param.get(\"files\")\n",
    "\n",
    "    if p_chunk_size == CHUNK_SIZE and p_chunk_overlap == CHUNK_OVERLAP and p_sources_dir == SOURCES_DIR:\n",
    "        removed_files = list(set(p_files) - set(files))\n",
    "        new_files = list(set(files) - set(p_files))\n",
    "        if removed_files:\n",
    "            print(f\"Les fichiers suivants ne sont plus pr√©sents : {removed_files}\")\n",
    "            nb_cleaned = chunk_eraser(removed_files)\n",
    "            print(f\"{nb_cleaned} fichiers de chunks ont √©t√© supprim√©s (dans '{OUTPUT_DIR}'/).\")\n",
    "            param[\"files\"] = [f for f in p_files if f not in removed_files]\n",
    "            write_json(os.path.join(OUTPUT_DIR, \"param.json\"), param)\n",
    "        if not new_files:\n",
    "            print(f\"Aucun nouveaux fichier n'a √©t√© d√©tect√© (dans '{SOURCES_DIR}/').\")\n",
    "        else:\n",
    "            print(f\"{len(new_files)} nouveaux fichiers ont √©t√© d√©tect√©s : {new_files} (dans '{SOURCES_DIR}/').\")\n",
    "        return new_files, param[\"files\"]\n",
    "\n",
    "def chunk_eraser(diff):\n",
    "    nb_cleaned = 0\n",
    "    for file in diff:\n",
    "        prefix = os.path.splitext(file)[0]\n",
    "        for f in os.listdir(OUTPUT_DIR):\n",
    "            if f.startswith(prefix) and f.endswith(\".txt\"):\n",
    "                file_path = os.path.join(OUTPUT_DIR, f)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    nb_cleaned += 1\n",
    "    return nb_cleaned\n",
    "\n",
    "files = [f for f in os.listdir(SOURCES_DIR) if f.endswith(\".txt\")]\n",
    "old_files = []\n",
    "if os.path.isfile(os.path.join(OUTPUT_DIR, \"param.json\")):\n",
    "    files, old_files = chunk_cleaner(files)\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "chunk_files(files, old_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6c14",
   "metadata": {},
   "source": [
    "# Cr√©ation de la base de donn√©es vectorielle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80a40b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture des chunks...\n",
      "G√©n√©ration des embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:15<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion dans la base vectorielle...\n",
      "508 chunks ajout√©s √† la base vectorielle dans 'vector_db/all_MiniLM_L6_v2'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "CHUNKS_NAME = \"CS_1000_CO_200\"\n",
    "CHUNKS_DIR = f\"chunks/{CHUNKS_NAME}\"  # Dossier contenant les chunks\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_DB_DIR = f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\"\n",
    "\n",
    "# Init Chroma\n",
    "client = chromadb.PersistentClient(path=VECTOR_DB_DIR)\n",
    "collection = client.get_or_create_collection(CHUNKS_NAME)\n",
    "\n",
    "# Embedding model\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Lecture des chunks\n",
    "def load_chunks():\n",
    "    chunks = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    for file in os.listdir(CHUNKS_DIR):\n",
    "        if file.endswith(\".txt\"):\n",
    "            path = os.path.join(CHUNKS_DIR, file)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                chunks.append(content)\n",
    "                metadatas.append({\"filename\": file})\n",
    "                ids.append(str(uuid.uuid4()))  # identifiant unique\n",
    "    return chunks, metadatas, ids\n",
    "\n",
    "# G√©n√©ration + insertion\n",
    "def embed_and_store():\n",
    "    print(\"Lecture des chunks...\")\n",
    "    texts, metadatas, ids = load_chunks()\n",
    "\n",
    "    print(\"G√©n√©ration des embeddings...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True).tolist()\n",
    "\n",
    "    print(\"Insertion dans la base vectorielle...\")\n",
    "    collection.add(\n",
    "        documents=texts,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    print(f\"{len(texts)} chunks ajout√©s √† la base vectorielle dans '{VECTOR_DB_DIR}'.\")\n",
    "\n",
    "embed_and_store()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e864d304",
   "metadata": {},
   "source": [
    "### Affichage des collections existantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da70a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom : CS_1000_CO_100, ID : 1f389081-73c5-4daf-b838-bf88aa24ab5f\n",
      "Nom : CS_1000_CO_150, ID : 4ee75404-7ce2-49f3-be03-f455ef30b653\n",
      "Nom : CS_800_CO_80, ID : 53fcc4cb-7fb3-4cc9-8b49-e0eea43e1188\n",
      "Nom : CS_300_CO_60, ID : 9cae8747-44df-47d3-8054-851d6d71bad4\n",
      "Nom : CS_300_CO_30, ID : bf0699be-251c-45f6-9656-0a7d9c38bac6\n",
      "Nom : CS_800_CO_160, ID : c8e7ab35-faf7-4d49-bcab-a43898a80a8d\n",
      "Nom : CS_500_CO_100, ID : d2e7a108-12b2-4a48-8f9f-e83c371e60f5\n",
      "Nom : CS_1000_CO_200, ID : dd9a73c8-df80-4627-bd14-4d734ae5108f\n",
      "Nom : CS_500_CO_50, ID : e21c2070-abd4-4b08-8f56-efa70d4e1a00\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\")\n",
    "collections = client.list_collections()\n",
    "\n",
    "# client.delete_collection(\"CS_800_CO_160\")\n",
    "\n",
    "for col in collections:\n",
    "    print(f\"Nom : {col.name}, ID : {col.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d215ee5",
   "metadata": {},
   "source": [
    "# Test du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f20f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des passages les plus pertinents...\n",
      "\n",
      "\n",
      "--- R√©sultat 1 (Warhammer 4 - Livre de base_chunk_268.txt) - Similarit√© : 0.48% (0.995) ---\n",
      "√† pleine \n",
      "vitesse vers l'ennemi le plus proche que vous pouvez voir afin \n",
      "d'attaquer. En g√©n√©ral, la seule action que vous pouvez faire est \n",
      "un Test de Comp√©tence d'Armes ou un Test d'Athl√©tisme pour \n",
      "atteindre un ennemi plus rapidement. De plus, vous pouvez faire \n",
      "un Test de M√™l√©e d'Action Gratuit √† chaque Round pendant que \n",
      "vous lancez tout ce que vous avez dans vos attaques. Enfin, vous \n",
      "gagnez un bonus de +1 Bonus de Force, telle est votre f√©rocit√©. \n",
      "Vous restez dans la Fr√©n√©sie jusqu'√† ce que tous les ennemis dans \n",
      "votre ligne de vue soient pacifi√©s, ou jusqu'√† ce que vous receviez \n",
      "la condition √âtourdie ou Inconsciente . Une fois votre Fr√©n√©sie \n",
      "termin√©e, vous recevez imm√©diatement un √©tat de Fatigue .  \n",
      "Haine (Cible)\n",
      "Vous √™tes consum√© par la Haine pour la Cible, qui est \n",
      "normalement un groupe de personnes ou de cr√©atures, telles \n",
      "que'Hochlanders','Pieuvres des tourbi√®res', ou'Esclaves'. Vous \n",
      "n'aurez jamais \n",
      "d'interaction sociale avec quelqu'un ou quelque \n",
      "chose que vous d√©teste\n",
      "\n",
      "--- R√©sultat 2 (Warhammer 4 - Livre de base_chunk_231.txt) - Similarit√© : 0.16% (0.998) ---\n",
      "attaque \n",
      "en combat rapproch√© gagne un bonus de +10 pour vous \n",
      "toucher.\n",
      "Une Condition Aveugl√© est supprim√©e √† la fin de chaque \n",
      "Round.\n",
      "Briser\n",
      "Vous √™tes terrifi√©, vaincu, paniqu√© ou convaincu que vous allez \n",
      "mourir. A votre tour, votre Mouvement et votre Action doivent \n",
      "√™treutilis√©s pour fuir aussi vite que possible jusqu'√† ce que vous soyez \n",
      "dans une bonne cachette hors de la vue de l'ennemi ; alors \n",
      "vous pouvez utiliser votre Action sur une comp√©tence qui \n",
      "vous permet de vous cacher plus efficacement. Vous recevez \n",
      "√©galement une p√©nalit√© de -10 √† tous les tests qui n'impliquent \n",
      "pas de courir et de se cacher.\n",
      "Vous ne pouvez pas faire de Test de Ralliement pour √©viter d'√™tre \n",
      "Bris√© si vous √™tes Engag√© avec un ennemi (voir page 159). Si vous \n",
      "n'√™tes pas engag√©, √† la fin de chaque Round, vous pouvez \n",
      "tenter un Test de Calme pour √©liminer une Condition Bris√©e,  \n",
      "chaque SL √©liminant une Condition  Bris√©e  suppl√©mentaire, et la \n",
      "Difficult√© d√©termin√©e par les circonstances dans lesquelles vou\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "CHUNKS_NAME = \"CS_1000_CO_200\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_DB_DIR = VECTOR_DB_DIR = f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\"\n",
    "TOP_K = 10  # nombre de r√©sultats √† retourner\n",
    "\n",
    "# Initialisation du client et du mod√®le\n",
    "client = chromadb.PersistentClient(path=VECTOR_DB_DIR)\n",
    "collection = client.get_collection(CHUNKS_NAME)\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def ask_question():\n",
    "    # question = input(\"Pose ta question : \").strip()\n",
    "    # if not question:\n",
    "    #     print(\"Tu dois poser une question.\")\n",
    "    #     return\n",
    "    question = \"Je souhaite faire une attaque √† l'√©p√©e sur un ennemi, comment dois-je proc√©der ?\"\n",
    "\n",
    "    print(\"Recherche des passages les plus pertinents...\\n\")\n",
    "\n",
    "    # Embedding de la question\n",
    "    query_embedding = model.encode([question])[0].tolist()\n",
    "\n",
    "    # Recherche vectorielle\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Filtrer les r√©sultats pour exclure ceux avec une distance >= 1\n",
    "    filtered_docs = []\n",
    "    filtered_metas = []\n",
    "    filtered_dists = []\n",
    "    for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "        if dist < 1:\n",
    "            filtered_docs.append(doc)\n",
    "            filtered_metas.append(meta)\n",
    "            filtered_dists.append(dist)\n",
    "\n",
    "\n",
    "\n",
    "    # Affichage des r√©sultats filtr√©s\n",
    "    for i, (doc, meta, dist) in enumerate(zip(filtered_docs, filtered_metas, filtered_dists)):\n",
    "        print(f\"\\n--- R√©sultat {i+1} ({meta['filename']}) - Similarit√© : {round((1 - dist) * 100, 2)}% ({round(dist, 3)}) ---\")\n",
    "        print(doc.strip()[:1000])\n",
    "\n",
    "    filtered_results = {\n",
    "        \"documents\": [filtered_docs],\n",
    "        \"metadatas\": [filtered_metas],\n",
    "        \"distances\": [filtered_dists]\n",
    "    }\n",
    "    return filtered_results, question\n",
    "        \n",
    "RESULTS, QUESTION = ask_question()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf785fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- R√©ponse de l'IA ---\n",
      "Pour effectuer une attaque √† l'√©p√©e contre un adversaire, vous devrez tout d'abord v√©rifier si vous √™tes dans la port√©e de l'√©p√©e et que le cible soit visible. Si cela est le cas, vous pouvez faire un test de comp√©tence des armes pour effectuer votre attaque. En tant que personnage en fr√©n√©sie, vous b√©n√©ficiez d'un bonus de +1 force.\n",
      "\n",
      "   De plus, vous gagnez un bonus de +1 pour toutes vos attaques contre le groupe ha√Øss√© si vous avez la haine pour cette cible. Enfin, vous pouvez effectuer un test de melee d'action gratuit √† chaque tour pour lancer une attaque suppl√©mentaire sans action compl√®te.\n",
      "\n",
      "   Si vous avez la pr√©f√©rence ou les pr√©jug√©s contre le groupe ennemis, vous subirez une p√©nalit√© de -10 √† tous vos tests de sociabilit√© envers ce groupe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_llm_with_context_ollam(question, context, model=\"mistral\"):\n",
    "    context = \"\\n\\n\".join(RESULTS[\"documents\"][0])\n",
    "    prompt = f\"\"\"Tu es un assistant intelligent portant sur le jeu de role sur table Warhammer Fantasy.\n",
    "    Ton role est de r√©pondre aux questions concernant les r√®gles du jeu ainsi que sur l'univers de Warhammer Fantasy.\n",
    "    Tu dois r√©pondre de mani√®re concise et pr√©cise, en te basant sur les informations fournies dans le contexte ci-dessous.\n",
    "\n",
    "    ### Contexte :\n",
    "\n",
    "    {context}\n",
    "\n",
    "\n",
    "    ### Question : \n",
    "\n",
    "    {question}\n",
    "    \n",
    "\n",
    "    ### R√©ponse :\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data = response.json()\n",
    "    return data[\"response\"]\n",
    "\n",
    "question = QUESTION\n",
    "context = \"\\n\\n\".join(RESULTS[\"documents\"][0])\n",
    "answer = get_llm_with_context_ollam(question, context)\n",
    "\n",
    "print(f\"\\n--- R√©ponse de l'IA ---\\n{answer.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c465257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flux extrait dans c:\\Projets\\LLM assistant\\sources/image704.jp2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_stream_from_pdf(pdf_path, obj_num, output_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    start_marker = f\"{obj_num} 0 obj\".encode()\n",
    "    stream_marker = b'stream\\n'\n",
    "    endstream_marker = b'endstream'\n",
    "\n",
    "    start_obj = data.find(start_marker)\n",
    "    if start_obj == -1:\n",
    "        print(\"Objet non trouv√©\")\n",
    "        return\n",
    "\n",
    "    start_stream = data.find(stream_marker, start_obj) + len(stream_marker)\n",
    "    end_stream = data.find(endstream_marker, start_stream)\n",
    "\n",
    "    stream_data = data[start_stream:end_stream]\n",
    "\n",
    "    with open(output_path, 'wb') as out:\n",
    "        out.write(stream_data)\n",
    "\n",
    "    print(f\"Flux extrait dans {output_path}\")\n",
    "\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "extract_stream_from_pdf(sources_path + '/output_uncompressed.pdf', 704, sources_path + '/image704.jp2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM1)",
   "language": "python",
   "name": "llm1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
