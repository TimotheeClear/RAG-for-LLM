{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7dcb96",
   "metadata": {},
   "source": [
    "# Chargement des fichiers sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afa08c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargé : Warhammer 4 - Livre de base.pdf (67641992 caractères) .pdf)\n",
      "Conversion du PDF Warhammer 4 - Livre de base.pdf en TXT...\n",
      "Fichier TXT créé : Warhammer 4 - Livre de base.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Import Files \n",
    "def file_loader(folder):\n",
    "    files = []\n",
    "    for i, file_name in enumerate(os.listdir(folder)):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            extension = os.path.splitext(file_name)[1].lower()\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                files.append({\n",
    "                    'name': file_name,\n",
    "                    'path': file_path,\n",
    "                    'extension': extension,\n",
    "                    'content': f.read(),\n",
    "                })\n",
    "            print(f\"Chargé : {files[i]['name']} ({len(files[i]['content'])} caractères) {files[i]['extension']})\")\n",
    "    return files\n",
    "\n",
    "# Convert PDF to TXT\n",
    "def pdf_to_txt(file, output_folder_path):\n",
    "    \"\"\"\n",
    "    Convert a PDF file (already loaded as a dict from file_loader) to TXT and save it in output_folder.\n",
    "    Returns the path to the TXT file.\n",
    "    \"\"\"\n",
    "    txt_name = os.path.splitext(file['name'])[0] + '.txt'\n",
    "    txt_path = os.path.join(output_folder_path, txt_name)\n",
    "    if file['extension'] == '.pdf':\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Conversion du PDF {file['name']} en TXT...\")\n",
    "            with open(file['path'], 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f_txt:\n",
    "                f_txt.write(text)\n",
    "            print(f\"Fichier TXT créé : {os.path.splitext(file['name'])[0]}.txt\")\n",
    "        else:\n",
    "            print(f\"Le fichier TXT {txt_name} existe déjà, pas de conversion nécessaire.\")\n",
    "        return txt_path\n",
    "    print(f\"Le fichier {file['name']} n'est pas un PDF, pas de conversion effectuée.\")\n",
    "    return None\n",
    "\n",
    "sources_path = os.path.join(os.getcwd(), \"sources\")\n",
    "transformed_sources_path = os.path.join(os.getcwd(), \"transformed_sources\")\n",
    "files = file_loader(sources_path)\n",
    "for file in files:\n",
    "    if(file['extension'] == '.pdf'):\n",
    "        pdf_to_txt(file, transformed_sources_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017fc49",
   "metadata": {},
   "source": [
    "# Découpage des fichiers txt en chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af218f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Découpage de Warhammer 4 - Livre de base.txt en chunk de 1000 tokens...\n",
      "1 fichiers ont été découpés en chunks (dans 'chunks/CS_1000_CO_200/').\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "# Paramètres des chunks\n",
    "SOURCES_DIR = \"transformed_sources\"                             # Dossier contenant les fichiers texte à découper\n",
    "CHUNK_SIZE = 1000                                                # Nombre de tokens par chunk\n",
    "CHUNK_OVERLAP = 200                                              # Nombre de tokens de chevauchement\n",
    "OUTPUT_DIR = f\"chunks/CS_{CHUNK_SIZE}_CO_{CHUNK_OVERLAP}\"       # Dossier de sortie pour les chunks\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")     # Utiliser l'encodeur de tokens de OpenAI (compatible Mistral)\n",
    "\n",
    "# Initialiser le découpeur de texte\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    encoding_name=\"cl100k_base\",\n",
    ")\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(path, config):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def chunk_files(new_files, old_files):\n",
    "    i = 0\n",
    "    for i, new_file in enumerate(new_files, start=1):\n",
    "        print(f\"Découpage de {new_file} en chunk de {CHUNK_SIZE} tokens...\")\n",
    "        chunk_file(os.path.join(SOURCES_DIR, new_file))\n",
    "\n",
    "    print(f\"{i} fichiers ont été découpés en chunks (dans '{OUTPUT_DIR}/').\")\n",
    "    new_param = {\n",
    "        \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "        \"CHUNK_OVERLAP\": CHUNK_OVERLAP,\n",
    "        \"SOURCE_DIR\": SOURCES_DIR,\n",
    "        \"files\": old_files + new_files,\n",
    "    }\n",
    "    write_json(os.path.join(OUTPUT_DIR, \"param.json\"), new_param)\n",
    "\n",
    "\n",
    "def chunk_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        with open(os.path.join(OUTPUT_DIR, f\"{filename}_chunk_{i}.txt\"), \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(chunk)\n",
    "\n",
    "def chunk_cleaner(files):\n",
    "    param = read_json(os.path.join(OUTPUT_DIR, \"param.json\"))\n",
    "\n",
    "    p_chunk_size = param.get(\"CHUNK_SIZE\")\n",
    "    p_chunk_overlap = param.get(\"CHUNK_OVERLAP\")\n",
    "    p_sources_dir = param.get(\"SOURCE_DIR\")\n",
    "    p_files = param.get(\"files\")\n",
    "\n",
    "    if p_chunk_size == CHUNK_SIZE and p_chunk_overlap == CHUNK_OVERLAP and p_sources_dir == SOURCES_DIR:\n",
    "        removed_files = list(set(p_files) - set(files))\n",
    "        new_files = list(set(files) - set(p_files))\n",
    "        if removed_files:\n",
    "            print(f\"Les fichiers suivants ne sont plus présents : {removed_files}\")\n",
    "            nb_cleaned = chunk_eraser(removed_files)\n",
    "            print(f\"{nb_cleaned} fichiers de chunks ont été supprimés (dans '{OUTPUT_DIR}'/).\")\n",
    "            param[\"files\"] = [f for f in p_files if f not in removed_files]\n",
    "            write_json(os.path.join(OUTPUT_DIR, \"param.json\"), param)\n",
    "        if not new_files:\n",
    "            print(f\"Aucun nouveaux fichier n'a été détecté (dans '{SOURCES_DIR}/').\")\n",
    "        else:\n",
    "            print(f\"{len(new_files)} nouveaux fichiers ont été détectés : {new_files} (dans '{SOURCES_DIR}/').\")\n",
    "        return new_files, param[\"files\"]\n",
    "\n",
    "def chunk_eraser(diff):\n",
    "    nb_cleaned = 0\n",
    "    for file in diff:\n",
    "        prefix = os.path.splitext(file)[0]\n",
    "        for f in os.listdir(OUTPUT_DIR):\n",
    "            if f.startswith(prefix) and f.endswith(\".txt\"):\n",
    "                file_path = os.path.join(OUTPUT_DIR, f)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    nb_cleaned += 1\n",
    "    return nb_cleaned\n",
    "\n",
    "files = [f for f in os.listdir(SOURCES_DIR) if f.endswith(\".txt\")]\n",
    "old_files = []\n",
    "if os.path.isfile(os.path.join(OUTPUT_DIR, \"param.json\")):\n",
    "    files, old_files = chunk_cleaner(files)\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "chunk_files(files, old_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6c14",
   "metadata": {},
   "source": [
    "# Création de la base de données vectorielle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80a40b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture des chunks...\n",
      "Génération des embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:15<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion dans la base vectorielle...\n",
      "508 chunks ajoutés à la base vectorielle dans 'vector_db/all_MiniLM_L6_v2'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "CHUNKS_NAME = \"CS_1000_CO_200\"\n",
    "CHUNKS_DIR = f\"chunks/{CHUNKS_NAME}\"  # Dossier contenant les chunks\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_DB_DIR = f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\"\n",
    "\n",
    "# Init Chroma\n",
    "client = chromadb.PersistentClient(path=VECTOR_DB_DIR)\n",
    "collection = client.get_or_create_collection(CHUNKS_NAME)\n",
    "\n",
    "# Embedding model\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# Lecture des chunks\n",
    "def load_chunks():\n",
    "    chunks = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    for file in os.listdir(CHUNKS_DIR):\n",
    "        if file.endswith(\".txt\"):\n",
    "            path = os.path.join(CHUNKS_DIR, file)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                chunks.append(content)\n",
    "                metadatas.append({\"filename\": file})\n",
    "                ids.append(str(uuid.uuid4()))  # identifiant unique\n",
    "    return chunks, metadatas, ids\n",
    "\n",
    "# Génération + insertion\n",
    "def embed_and_store():\n",
    "    print(\"Lecture des chunks...\")\n",
    "    texts, metadatas, ids = load_chunks()\n",
    "\n",
    "    print(\"Génération des embeddings...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True).tolist()\n",
    "\n",
    "    print(\"Insertion dans la base vectorielle...\")\n",
    "    collection.add(\n",
    "        documents=texts,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    print(f\"{len(texts)} chunks ajoutés à la base vectorielle dans '{VECTOR_DB_DIR}'.\")\n",
    "\n",
    "embed_and_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da70a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom : CS_1000_CO_100, ID : 1f389081-73c5-4daf-b838-bf88aa24ab5f\n",
      "Nom : CS_1000_CO_150, ID : 4ee75404-7ce2-49f3-be03-f455ef30b653\n",
      "Nom : CS_800_CO_80, ID : 53fcc4cb-7fb3-4cc9-8b49-e0eea43e1188\n",
      "Nom : CS_300_CO_60, ID : 9cae8747-44df-47d3-8054-851d6d71bad4\n",
      "Nom : CS_300_CO_30, ID : bf0699be-251c-45f6-9656-0a7d9c38bac6\n",
      "Nom : CS_800_CO_160, ID : c8e7ab35-faf7-4d49-bcab-a43898a80a8d\n",
      "Nom : CS_500_CO_100, ID : d2e7a108-12b2-4a48-8f9f-e83c371e60f5\n",
      "Nom : CS_1000_CO_200, ID : dd9a73c8-df80-4627-bd14-4d734ae5108f\n",
      "Nom : CS_500_CO_50, ID : e21c2070-abd4-4b08-8f56-efa70d4e1a00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = chromadb.PersistentClient(path=f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\")\n",
    "collections = client.list_collections()\n",
    "\n",
    "# client.delete_collection(\"CS_800_CO_160\")\n",
    "\n",
    "for col in collections:\n",
    "    print(f\"Nom : {col.name}, ID : {col.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d215ee5",
   "metadata": {},
   "source": [
    "# Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f20f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timot\\anaconda3\\envs\\LLM1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des passages les plus pertinents...\n",
      "\n",
      "\n",
      "--- Résultat 1 (Warhammer 4 - Livre de base_chunk_441.txt) - Similarité : 13.25% (0.867) ---\n",
      "iquée principalement par \n",
      "les commerçants ou les préposés au péage qui ont accès à une \n",
      "grande quantité de l'argent des autres personnes. Les limailles \n",
      "d'or et d'argent sont ensuite fondues et vendues à des bijoutiers, \n",
      "à des faussaires ou à des receleurs. Les pièces découpées peuvent \n",
      "être détectées à l'aide de la Compétence Évaluer  ; plus il y a de \n",
      "pièces découpées, plus le Test est facile. \n",
      "LE CRITÈRE NULN\n",
      "Altdorf est peut-être l'actuelle capitale du Reikland et de \n",
      "l'Empire, mais les normes monétaires sont établies dans la \n",
      "ville-état de Nuln au sud. Historiquement, Nuln était la \n",
      "capitale de l'Empire jusqu'à ce que la Maison Holswig-\n",
      "Schliestein restaure le trône d'Altdorf il y a un siècle, et de \n",
      "nombreuses institutions impériales l'habitent encore. Le Nuln \n",
      "Standard ne régit que le poids et la métallurgie des pièces \n",
      "de monnaie, et non l'imagerie des timbres, qui varie \n",
      "considérablement selon les provinces.\n",
      "XI LE GUIDE DU CONSOMMATEUR\n",
      "289\n",
      "WARHAMMER FANTASY ROLPLAY\n",
      "ALLER AU MA\n",
      "\n",
      "--- Résultat 2 (Warhammer 4 - Livre de base_chunk_13.txt) - Similarité : 10.22% (0.898) ---\n",
      "ait officiellement votre droit d’heriter de ses terres et de ses \n",
      "titres dans le Reikland, et vous ordonne de le soutenir. Votre exil dans les principautes frontalieres touche àsa fin, et vous devez vous preparer \n",
      "immediatement pour votre depart. \n",
      "Dans sa sagesse, votre pere a employe une escorte armee pour l'aider. C'est moi qui commande et, si Sigmar le veut, nous arriverons dans un jour \n",
      "près de Sonnestill. Vous partirez avec nous pour la capitale imperiale le lendemain matin. Certes, de telles nouvelles rempliront votre jeune cœur de \n",
      "joie, car vous serez enfin temoins de la splendeur de la plus grande ville de l'Ancien Monde : Altdorf le magnifique, siege de l'empereur Karl-\n",
      "Franz Ier et le Grand Theogoniste de Sigmar, et residence actuelle de la Maison Siert, où votre pere fait cour non loin du zoo imperial sur la \n",
      "Colline Goellner.\n",
      "Je prendrai aussi en main votre education. Ceux qui conseillent votre pere pensent que les cours que vous recevez actuellement du colonel Sievers, bi\n",
      "\n",
      "--- Résultat 3 (Warhammer 4 - Livre de base_chunk_395.txt) - Similarité : 2.2% (0.978) ---\n",
      "ante de l'Empire et sur son peuple qui travaille dur et qui est uni par le culte du Roi-Dieu \n",
      "Sigmar. Admirez les comtes électeurs du Reikland qui ont dirigé le glorieux Empire pendant près d'un siècle, et leur généreux mécénat qui \n",
      "enrichit notre patrie sans mesure. Admirez l'impressionnant réseau de canaux du Reikland, ses vastes travaux routiers, son agriculture \n",
      "sophistiquée et sa classe mercantile en plein essor, qui confirment le Reikland comme un véritable joyau de l'Empire. \n",
      "Émerveillez-vous devant ce royaume impérial le plus riche et le plus cosmopolite de tous, et voyez qu'il s'agit d'une puissance culturelle, \n",
      "magique et académique sans égale, attirant les artisans, sorciers et érudits les plus impressionnants dans ses nombreuses institutions savantes, \n",
      "consolidant sa réputation comme le plus grand domaine du vieux monde.\n",
      "En vérité, naître Reiklander, c'est naître béni par les Dieux Eux-même. Rendez grâce à Sigmar et soyez loué.\n",
      "- Les paroles de Sainte Mère Halma Habermann d\n",
      "\n",
      "--- Résultat 4 (Warhammer 4 - Livre de base_chunk_414.txt) - Similarité : 0.41% (0.996) ---\n",
      "eter IV, \n",
      "en se réformant en tant que désert avec \n",
      "Marienburg comme capitale. Utilisant les \n",
      "lois anti-corruption mises en place par \n",
      "Magnus le Pieux presque cent ans plus tôt, \n",
      "les électeurs comptent destituer Dieter dans le \n",
      "scandale qui en résulta. Il est remplacé par \n",
      "le Grand Prince Wilhelm de la Maison \n",
      "Holswig-Schliestein du Reikland, qui est \n",
      "nommé Empereur Wilhelm III, à sa \n",
      "succession à la tête de l'Empire impérial qui \n",
      "règne encore aujourd'hui.\n",
      "La bataille du marais Grootscher. \n",
      "L'empereur Guillaume III, sous la pression \n",
      "des Comtes Électeurs pour répondre à la \n",
      "scission du Wasteland de l'Empire, \n",
      "rassemble les armées d’État pour envahir \n",
      "Marienburg. À la fin de l'automne, les \n",
      "forces opposées se rencontrent au marais \n",
      "Grootscher, juste à l'extérieur de Siert, et \n",
      "l'Empire est mis en déroute par la marine \n",
      "avancée de Marienburg, des mercenaires et \n",
      "des milices bien entraînés et la magie des \n",
      "alliés Haute-Elfes du Wasteland. A \n",
      "contrecœur, Wilhelm reconnaît verbalement \n",
      "l'ind\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "CHUNKS_NAME = \"CS_1000_CO_200\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_DB_DIR = VECTOR_DB_DIR = f\"vector_db/{EMBEDDING_MODEL.replace('-', '_')}\"\n",
    "TOP_K = 10  # nombre de résultats à retourner\n",
    "\n",
    "# Initialisation du client et du modèle\n",
    "client = chromadb.PersistentClient(path=VECTOR_DB_DIR)\n",
    "collection = client.get_collection(CHUNKS_NAME)\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def ask_question():\n",
    "    # question = input(\"Pose ta question : \").strip()\n",
    "    # if not question:\n",
    "    #     print(\"Tu dois poser une question.\")\n",
    "    #     return\n",
    "    question = \"Quel est la capitale de l'Empire ?\"\n",
    "\n",
    "    print(\"Recherche des passages les plus pertinents...\\n\")\n",
    "\n",
    "    # Embedding de la question\n",
    "    query_embedding = model.encode([question])[0].tolist()\n",
    "\n",
    "    # Recherche vectorielle\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Filtrer les résultats pour exclure ceux avec une distance >= 1\n",
    "    filtered_docs = []\n",
    "    filtered_metas = []\n",
    "    filtered_dists = []\n",
    "    for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "        if dist < 1:\n",
    "            filtered_docs.append(doc)\n",
    "            filtered_metas.append(meta)\n",
    "            filtered_dists.append(dist)\n",
    "\n",
    "\n",
    "\n",
    "    # Affichage des résultats filtrés\n",
    "    for i, (doc, meta, dist) in enumerate(zip(filtered_docs, filtered_metas, filtered_dists)):\n",
    "        print(f\"\\n--- Résultat {i+1} ({meta['filename']}) - Similarité : {round((1 - dist) * 100, 2)}% ({round(dist, 3)}) ---\")\n",
    "        print(doc.strip()[:1000])\n",
    "\n",
    "    filtered_results = {\n",
    "        \"documents\": [filtered_docs],\n",
    "        \"metadatas\": [filtered_metas],\n",
    "        \"distances\": [filtered_dists]\n",
    "    }\n",
    "    return filtered_results, question\n",
    "        \n",
    "RESULTS, QUESTION = ask_question()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf785fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Réponse de l'IA ---\n",
      "Altdorf est la capitale de l'Empire dans le scénario décrit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_llm_with_context_ollam(question, context, model=\"mistral\"):\n",
    "    context = \"\\n\\n\".join(RESULTS[\"documents\"][0])\n",
    "    prompt = f\"\"\"Tu es un assistant intelligent portant sur le jeu de role sur table Warhammer Fantasy.\n",
    "    Ton role est de répondre aux questions concernant les règles du jeu ainsi que sur l'univers de Warhammer Fantasy.\n",
    "    Tu dois répondre de manière concise et précise, en te basant sur les informations fournies dans le contexte ci-dessous.\n",
    "\n",
    "    ### Contexte :\n",
    "\n",
    "    {context}\n",
    "\n",
    "\n",
    "    ### Question : \n",
    "\n",
    "    {question}\n",
    "\n",
    "    ### Réponse :\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data = response.json()\n",
    "    return data[\"response\"]\n",
    "\n",
    "question = QUESTION\n",
    "context = \"\\n\\n\".join(RESULTS[\"documents\"][0])\n",
    "answer = get_llm_with_context_ollam(question, context)\n",
    "\n",
    "print(f\"\\n--- Réponse de l'IA ---\\n{answer.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8b477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM1)",
   "language": "python",
   "name": "llm1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
